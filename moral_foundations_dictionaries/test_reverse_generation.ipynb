{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.utils import io\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "import csv\n",
    "\n",
    "from dataset import CongressDataset\n",
    "from token_map import TokenMap, create_re_from_formatted_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn;\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "NUM_EPOCHS = 30\n",
    "DICTIONARY_SIMILARITY_THRESHOLD = 0.5\n",
    "WORD_MIN_COUNT = 100\n",
    "NUM_RECURRENCES = 3\n",
    "\n",
    "DICT_ID = \"reverse_continuous_training_v1\"\n",
    "DICTIONARY_SAVE_PATH = os.path.join(os.path.curdir, \"data\", DICT_ID, \"json_dicts\")\n",
    "os.makedirs(DICTIONARY_SAVE_PATH, exist_ok=True)\n",
    "MODELS_SAVE_PATH = os.path.join(os.path.curdir, \"data\", DICT_ID, \"embedding_models\")\n",
    "os.makedirs(MODELS_SAVE_PATH, exist_ok=True)\n",
    "PRETRAINED_MODEL_NAME = \"pretrained_wiki_model.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Fine Tuning\n",
    "LARGE_CLUSTER_WEIGHT = 0.02\n",
    "NUM_ENSAMBLE_RERUNS = 1\n",
    "# NUM_CLUSTERS_TO_CHECK = range(2, 13)\n",
    "# NUM_CLUSTER_RERUNS = 20\n",
    "\n",
    "PREVIOUS_TERM_BOOST = 3\n",
    "NUM_RECURRENCE_BOOST = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMATTED_WIKI_TOKENS_PATH = os.path.join(os.path.curdir, \"data\", \"formatted_wiki_unique_tokens_corpus.json\")\n",
    "# if not os.path.exists(FORMATTED_WIKI_TOKENS_PATH):\n",
    "\n",
    "#     wiki_corpus = api.load(\"wiki-english-20171001\")\n",
    "\n",
    "#     unique_tokens = set()\n",
    "#     formatted_wiki_data = []\n",
    "#     for thing in tqdm(wiki_corpus):\n",
    "#         for section in thing[\"section_texts\"]:\n",
    "#             sentences = nltk.sent_tokenize(section)\n",
    "#             for sentence in sentences:\n",
    "#                 words = nltk.word_tokenize(sentence)\n",
    "#                 for word in words:\n",
    "#                     unique_tokens.add(word)\n",
    "\n",
    "#     wiki_json_format = {\"tokens\": list(unique_tokens)}\n",
    "\n",
    "#     del wiki_corpus\n",
    "\n",
    "#     with open(FORMATTED_WIKI_TOKENS_PATH, \"w\") as f:\n",
    "#         json.dump(wiki_json_format, f)\n",
    "# else:\n",
    "#     with open(FORMATTED_WIKI_TOKENS_PATH, \"r\") as f:\n",
    "#         wiki_json_format = json.load(f)\n",
    "#     unique_tokens = wiki_json_format[\"tokens\"]\n",
    "\n",
    "# del wiki_json_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token map from disk...\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\n",
    "        os.path.curdir, \"data\", \"cr_speech_sentences_with_speaker_and_date.csv\"\n",
    "    )\n",
    "FORMATTED_DICT_PATH = os.path.join(\n",
    "    os.path.curdir,\n",
    "    \"data\",\n",
    "    DICT_ID, \n",
    "    \"hand_curated\", \n",
    "    \"1873-1892_recursive_dict.json\"\n",
    ")\n",
    "token_map_load_path = os.path.join(os.path.curdir, \"data\", DICT_ID, \"token_map\")\n",
    "\n",
    "dictionary_re = create_re_from_formatted_dictionary(FORMATTED_DICT_PATH)\n",
    "\n",
    "token_map = TokenMap(DATA_PATH, token_map_load_path, dictionary_re=dictionary_re, additional_corpus=unique_tokens)\n",
    "\n",
    "del unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store transformed WIKI CORPUS\n",
    "\n",
    "TRANSFORMED_WIKI_CORPUS_PATH = os.path.join(os.path.curdir, \"data\", \"transformed_wiki_corpus.csv\")\n",
    "\n",
    "if not os.path.exists(TRANSFORMED_WIKI_CORPUS_PATH):\n",
    "    with open(TRANSFORMED_WIKI_CORPUS_PATH, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        wiki_corpus = api.load(\"wiki-english-20171001\")\n",
    "\n",
    "        for thing in tqdm(wiki_corpus):\n",
    "            for section in thing[\"section_texts\"]:\n",
    "                sentences = nltk.sent_tokenize(section)\n",
    "                for sentence in sentences:\n",
    "                    words = nltk.word_tokenize(sentence)\n",
    "                    writer.writerow([token_map.get_token_id_from_token(word.lower()) for word in words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_path = os.path.join(MODELS_SAVE_PATH, PRETRAINED_MODEL_NAME)\n",
    "\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    print(\"Loading pretrained model\")\n",
    "    model = gensim.models.Word2Vec.load(pretrained_model_path)\n",
    "else:\n",
    "    transformed_wiki_corpus = []\n",
    "    # wiki_corpus = api.load(\"wiki-english-20171001\")\n",
    "    MAX_WIKI_TRAIN_SIZE = 10_000_000\n",
    "    model = None\n",
    "\n",
    "    transformed_wiki_data = []\n",
    "    train_size = 0\n",
    "    with open(TRANSFORMED_WIKI_CORPUS_PATH, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in tqdm(enumerate(reader)):\n",
    "            transformed_wiki_data.append([int(token_id) for token_id in row])\n",
    "\n",
    "            if train_size >= MAX_WIKI_TRAIN_SIZE:\n",
    "                if model == None:\n",
    "                    print(\"Training model\")\n",
    "                    model = gensim.models.Word2Vec(transformed_wiki_data, workers=12, vector_size=EMBEDDING_SIZE, window=5, min_count=WORD_MIN_COUNT)\n",
    "                    model.save(pretrained_model_path)\n",
    "\n",
    "                else:\n",
    "                    print(\"Updating model\")\n",
    "                    model.build_vocab(transformed_wiki_corpus, update=True)\n",
    "                    model.train(transformed_wiki_corpus, total_examples=len(transformed_wiki_corpus), epochs=NUM_EPOCHS)\n",
    "                    model.save(pretrained_model_path)\n",
    "\n",
    "                train_size = 0\n",
    "                transformed_wiki_data = []\n",
    "            train_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[(1893, 1912), (1913, 1932), (1933, 1952), (1953, 1972), (1973, 1992), (1993, 2000), (2001, 2020)]\n"
     ]
    }
   ],
   "source": [
    "dictionary_time_periods = []\n",
    "\n",
    "start_year = 1873\n",
    "end_year = 2000\n",
    "\n",
    "\n",
    "temp_year = 1873\n",
    "while temp_year < end_year - 1:\n",
    "    dictionary_time_periods.append((temp_year, temp_year + 19))\n",
    "    temp_year += 20\n",
    "\n",
    "dictionary_time_periods[0] = (1993, 2000)\n",
    "dictionary_time_periods = [(2001, 2020)] + dictionary_time_periods\n",
    "dictionary_time_periods = sorted(dictionary_time_periods, reverse=False)\n",
    "# dictionary_time_periods.pop(0)\n",
    "dictionary_time_periods.pop(-2)\n",
    "\n",
    "\n",
    "print(len(dictionary_time_periods))\n",
    "print(dictionary_time_periods[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the starting dictionaries\n",
    "with open(FORMATTED_DICT_PATH, \"r\") as f:\n",
    "    curr_dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_gensim(data_item):\n",
    "    return data_item[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sim_score(word, dictionary_terms, model):\n",
    "    tot_sim_score = 0\n",
    "    num_valid_terms = 0\n",
    "\n",
    "    for term in dictionary_terms:\n",
    "        if term != word:\n",
    "            try:\n",
    "                specific_score = model.wv.similarity(word, term)\n",
    "                tot_sim_score += specific_score\n",
    "                num_valid_terms += 1\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "    return tot_sim_score / num_valid_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_sim_score(word, dictionary_terms, model):\n",
    "    all_sim_scores = []\n",
    "\n",
    "    for term in dictionary_terms:\n",
    "        if term != word:\n",
    "            try:\n",
    "                specific_score = model.wv.similarity(word, term)\n",
    "                all_sim_scores.append(specific_score)\n",
    "            except KeyError as e:\n",
    "                #os.path.join(\"data\", \"ensamble_fine_tuning\", \"original_dict.json\")\n",
    "                pass\n",
    "\n",
    "    all_sim_scores = sorted(all_sim_scores)\n",
    "    median_score = all_sim_scores[len(all_sim_scores) // 2]\n",
    "    if word in dictionary_terms:\n",
    "        # Boost the score of words that are already in the dictionary\n",
    "        # print(f\"Boosting {token_map.get_token_from_id(word)} from {median_score} to {median_score * (1 + PREVIOUS_TERM_BOOST)} because it is already in the dictionary\")\n",
    "        median_score = median_score * (1 + PREVIOUS_TERM_BOOST)\n",
    "\n",
    "    return median_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dictionary(model, dictionary_terms, previous_dictionary_size: int):\n",
    "     new_term_counts = {}\n",
    "     new_term_scores = {}\n",
    "\n",
    "     for term in dictionary_terms:\n",
    "          try:\n",
    "               top_matching_words = model.wv.most_similar(term, topn=previous_dictionary_size)\n",
    "\n",
    "               for (word, sim_score) in top_matching_words:\n",
    "                    # Keep track of max sim score for each word 0\n",
    "\n",
    "                    \n",
    "                    if not (word in new_term_scores):\n",
    "                         # Only needs to be calculated once\n",
    "                         # average_sim_score = calculate_average_sim_score(word, dictionary_terms, model)\n",
    "                         median_score = calculate_median_sim_score(word, dictionary_terms, model)\n",
    "                         new_term_scores[word] = median_score\n",
    "                    else:\n",
    "                         # Boost the score of words that appear more times\n",
    "                         # new_term_scores[word] = new_term_scores[word] * (1 + NUM_RECURRENCE_BOOST)\n",
    "                         original_score = new_term_scores[word]\n",
    "                         # median_score = calculate_median_sim_score(word, dictionary_terms, model)\n",
    "                         new_term_scores[word] = original_score * (1 + NUM_RECURRENCE_BOOST)\n",
    "                         \n",
    "\n",
    "                    # Keep track of how many times a word appears in the top matching words\n",
    "                    # if sim_score > DICTIONARY_SIMILARITY_THRESHOLD:\n",
    "                    #      if word not in new_term_counts:\n",
    "                    #           new_term_counts[word] = 0\n",
    "                    #      new_term_counts[word] += 1\n",
    "                         \n",
    "          except KeyError as e:\n",
    "               translated_term = token_map.get_token_from_id(term)\n",
    "               # print(f\"{translated_term}-{term}: {e}\")\n",
    "\n",
    "     # print(\"Previous dictionary reoccurrences:\")\n",
    "     # print({k: v for k, v in sorted(previous_dict_reocurrences.items(), key=lambda item: item[0])})\n",
    "     # Sort dictionaries and remove words that don't appear enough times\n",
    "     # new_term_counts = {k: v for k, v in sorted(new_term_counts.items(), key=lambda item: item[1], reverse=True) if v >= NUM_RECURRENCES}     \n",
    "     new_term_scores = {k: v for k, v in sorted(new_term_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "\n",
    "     # Compile the dictionary from the recurring words\n",
    "     final_dictionary_terms = set()\n",
    "     final_dictionary_scores = {}\n",
    "     # for term in new_term_counts.keys():\n",
    "     #      if len(final_dictionary_terms) >= previous_dictionary_size:\n",
    "     #           break\n",
    "     #      final_dictionary_terms.add(term)\n",
    "\n",
    "     # Add words based on similarity score until the dictionary is full\n",
    "     for term, score in new_term_scores.items():\n",
    "          if len(final_dictionary_terms) >= previous_dictionary_size:\n",
    "               break\n",
    "          final_dictionary_terms.add(term)\n",
    "          final_dictionary_scores[term] = score\n",
    "     \n",
    "     # Remove the unknown token if it is in the dictionary\n",
    "     # The unkown token is meaningless\n",
    "     if token_map.unkown_id in final_dictionary_terms:\n",
    "          final_dictionary_terms.remove(token_map.unkown_id)\n",
    "          final_dictionary_scores.pop(token_map.unkown_id)\n",
    "\n",
    "     # print(f\"Previous dictionary reocurrences: {sorted([token_map.get_token_from_id(term) for term in final_dictionary_terms if term in dictionary_terms]) }\")\n",
    "     # print(f\"Average dictionary_sim_score for with previous boost: {sum(final_dictionary_scores.values()) / len(final_dictionary_scores)}\")\n",
    "     # print(f\"Average score of top 25 terms: {sum(sorted(final_dictionary_scores.values(), reverse=True)[:25]) / 25}. There are {len(final_dictionary_scores)} terms in the dictionary.\")\n",
    "\n",
    "     return list(final_dictionary_terms), final_dictionary_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_of_scores_to_final_scores(list_scores_dict):\n",
    "    \"\"\"\n",
    "    Returns a list of terms translated to english sorted by their values in descending order\n",
    "    \"\"\"\n",
    "    final_scores = {}\n",
    "\n",
    "    total_num_recurrences = 0\n",
    "    for term, score_lists in list_scores_dict.items():\n",
    "        num_recurrences = len(score_lists)\n",
    "        if num_recurrences > 1:\n",
    "            total_num_recurrences += 1\n",
    "\n",
    "        # Each term with multiple occurences gets a percentage boost over the average score\n",
    "        # TODO: Consider using the median here instead of the average\n",
    "        # if num_recurrences > 1:\n",
    "        #     print(f\"Term {token_map.get_token_from_id(term)} has {num_recurrences} recurrences\")\n",
    "    \n",
    "        final_scores[term] = (sum(score_lists) / num_recurrences) * (1 + (num_recurrences * NUM_RECURRENCE_BOOST))\n",
    "        \n",
    "    final_dictionary = [token_map.get_token_from_id(k) for k, v in sorted(final_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "    final_scores = [v for k, v in sorted(final_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "    return final_dictionary, final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_ensamble_dicts(ensamble_dicts, desired_dict_sizes):\n",
    "    combined_dict = {}\n",
    "\n",
    "    for moral_foundation, dict_terms in ensamble_dicts.items():\n",
    "        # Convert list of scores to final scores\n",
    "        sorted_terms, sorted_scores = convert_list_of_scores_to_final_scores(dict_terms)\n",
    "        final_dict_terms = sorted_terms[:desired_dict_sizes[moral_foundation]]\n",
    "        final_dict_scores = sorted_scores[:desired_dict_sizes[moral_foundation]]\n",
    "\n",
    "        # print(f\"Final dictionary size for {moral_foundation}: {len(final_dict_terms)}\")\n",
    "        # print(f\"Average score for {moral_foundation}: {sum(final_dict_scores) / len(final_dict_scores)}\")\n",
    "\n",
    "        combined_dict[moral_foundation] = final_dict_terms\n",
    "\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time period is (1893, 1912).  1/7\n",
      "The current time period is (1913, 1932).  2/7\n",
      "The current time period is (1933, 1952).  3/7\n",
      "The current time period is (1953, 1972).  4/7\n",
      "The current time period is (1973, 1992).  5/7\n",
      "The current time period is (1993, 2000).  6/7\n",
      "The current time period is (2001, 2020).  7/7\n"
     ]
    }
   ],
   "source": [
    "for i, time_period in enumerate(dictionary_time_periods):\n",
    "    term_counts_and_scores_by_dictionary = {}\n",
    "    desired_dict_sizes = {}\n",
    "    num_trained_models = 0\n",
    "    tot_train_time = 0\n",
    "    print(f\"The current time period is {time_period}.  {i+1}/{len(dictionary_time_periods)}\")\n",
    "    model_save_path = os.path.join(MODELS_SAVE_PATH, f\"{time_period[0]}_{time_period[1]}.model\")\n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        # Load the dataset\n",
    "        middle_date = (time_period[0] + time_period[1]) // 2\n",
    "        first_time_period = (time_period[0], middle_date)\n",
    "        second_time_period = (middle_date, time_period[1])\n",
    "        dataset = CongressDataset(token_map=token_map, date_range=first_time_period)\n",
    "        dataset.map(format_data_for_gensim)\n",
    "\n",
    "        # start_time = time.time()\n",
    "        model.build_vocab(dataset.data, update=True)\n",
    "        model.train(dataset.data, epochs=NUM_EPOCHS, total_examples=len(dataset.data))\n",
    "\n",
    "        dataset = CongressDataset(token_map=token_map, date_range=second_time_period)\n",
    "        dataset.map(format_data_for_gensim)\n",
    "\n",
    "        # start_time = time.time()\n",
    "        model.build_vocab(dataset.data, update=True)\n",
    "        model.train(dataset.data, epochs=NUM_EPOCHS, total_examples=len(dataset.data))\n",
    "        # model = gensim.models.Word2Vec(dataset.data, vector_size=EMBEDDING_SIZE, window=5, min_count=WORD_MIN_COUNT, workers=12)\n",
    "        # tot_train_time += time.time() - start_time\n",
    "        model.save(model_save_path)\n",
    "        del dataset\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec.load(model_save_path)\n",
    "\n",
    "    for dict_name in curr_dicts.keys():\n",
    "        if dict_name not in term_counts_and_scores_by_dictionary:\n",
    "            term_counts_and_scores_by_dictionary[dict_name] = {}\n",
    "\n",
    "        # grab existing terms\n",
    "        dict_terms = curr_dicts[dict_name]\n",
    "        desired_dict_sizes[dict_name] = len(dict_terms)\n",
    "        translated_terms = [token_map.get_token_id_from_token(term) for term in dict_terms if token_map.get_token_id_from_token(term) in model.wv]\n",
    "\n",
    "        dictionary_embeddings = [embedding for embedding in model.wv[translated_terms]]\n",
    "\n",
    "        # Calculate the centroids of the clusters\n",
    "        # num_clusters, cluster_labels = calculate_cluster_centroids(dictionary_embeddings)\n",
    "        # cluster_distributions = calculate_cluster_distributions(cluster_labels, num_clusters)\n",
    "        # print(cluster_distributions)\n",
    "        # translated_terms_and_labels = list(zip(translated_terms, cluster_labels))\n",
    "\n",
    "        # Calculate the new dictionary terms\n",
    "        new_terms, new_term_scores = calculate_dictionary(model, translated_terms, len(dict_terms))\n",
    "\n",
    "        for term, score in new_term_scores.items():\n",
    "            if term not in term_counts_and_scores_by_dictionary[dict_name]:\n",
    "                term_counts_and_scores_by_dictionary[dict_name][term] = []\n",
    "            term_counts_and_scores_by_dictionary[dict_name][term].append(score)\n",
    "\n",
    "    curr_dicts = combine_ensamble_dicts(term_counts_and_scores_by_dictionary, desired_dict_sizes)\n",
    "\n",
    "    # Save the new completed dictionary\n",
    "    new_dict_name = f\"{time_period[0]}-{time_period[1]}_recursive_dict.json\"\n",
    "    new_dict_path = os.path.join(DICTIONARY_SAVE_PATH, new_dict_name)\n",
    "    with open(new_dict_path, \"w+\") as f:\n",
    "        json.dump(curr_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_dict = {}\n",
    "\n",
    "# for moral_foundation, dict_terms in term_counts_and_scores_by_dictionary.items():\n",
    "#     # Convert list of scores to final scores\n",
    "#     sorted_terms, sorted_scores = convert_list_of_scores_to_final_scores(dict_terms)\n",
    "#     # for term, score in zip(sorted_terms, sorted_scores):\n",
    "#     #     in_previous_dict = term in curr_dicts[moral_foundation]\n",
    "#     #     print(f\"{term}: {score}, in previous dict: {in_previous_dict}\")\n",
    "#     # break\n",
    "#     final_dict_terms = sorted_terms[:desired_dict_sizes[moral_foundation]]\n",
    "#     num_in_previous_dict = sum([1 for term in final_dict_terms if term in curr_dicts[moral_foundation]])\n",
    "#     print(f\"Number of terms in previous dict: {num_in_previous_dict}\")\n",
    "#     final_dict_scores = sorted_scores[:desired_dict_sizes[moral_foundation]]\n",
    "\n",
    "#     print(f\"Final dictionary size for {moral_foundation}: {len(final_dict_terms)}\")\n",
    "#     print(f\"Average score for {moral_foundation}: {sum(final_dict_scores) / len(final_dict_scores)}\")\n",
    "\n",
    "#     combined_dict[moral_foundation] = final_dict_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, time_period in enumerate(dictionary_time_periods[2:]):\n",
    "#     # Load the dataset\n",
    "#     dataset = CongressDataset(token_map=token_map, date_range=time_period)\n",
    "#     dataset.map(format_data_for_gensim)\n",
    "\n",
    "#     # Train a word2vec model and calculate important terms ENSAMBLE_RERUNS times\n",
    "#     term_counts_and_scores_by_dictionary = {}\n",
    "#     num_trained_models = 0\n",
    "#     tot_train_time = 0\n",
    "#     print(f\"The current time period is {time_period}.  {i+1}/{len(dictionary_time_periods)}\")\n",
    "#     p_bar = tqdm(range(NUM_ENSAMBLE_RERUNS), desc=\"Training models in an ensamble\")\n",
    "#     for x in p_bar:\n",
    "#         model_save_path = os.path.join(MODELS_SAVE_PATH, f\"{x}_{time_period[0]}_{time_period[1]}.model\")\n",
    "\n",
    "#         if os.path.exists(model_save_path):\n",
    "#             model = gensim.models.Word2Vec.load(model_save_path)\n",
    "#         else:\n",
    "#             start_time = time.time()\n",
    "#             model = gensim.models.Word2Vec(dataset.data, vector_size=EMBEDDING_SIZE, window=5, min_count=WORD_MIN_COUNT)\n",
    "#             tot_train_time += time.time() - start_time\n",
    "#             model.save(model_save_path)\n",
    "#             num_trained_models += 1\n",
    "#             p_bar.set_postfix_str(f\"Trained {num_trained_models} models. Average time {tot_train_time/num_trained_models} seconds\")\n",
    "\n",
    "#         continue\n",
    "\n",
    "#         # Create new dictionary for every dictionary in the current dictionaries\n",
    "#         print(\"Calculating new dictionary terms\")\n",
    "#         for dict_name in curr_dicts.keys():\n",
    "#             # grab existing terms\n",
    "#             dict_terms = curr_dicts[dict_name]\n",
    "#             translated_terms = [token_map.get_token_id_from_token(term) for term in dict_terms]\n",
    "\n",
    "#             #calclulate new terms\n",
    "#             new_term_ids = calculate_dictionary(model, translated_terms, previous_dictionary_size=len(dict_terms))\n",
    "#             new_terms = [token_map.get_token_from_id(term_id) for term_id in new_term_ids]\n",
    "#             new_terms_and_scores = [(term, 0) for term in new_terms]\n",
    "\n",
    "#             if dict_name not in term_counts_and_scores_by_dictionary:\n",
    "#                 term_counts_and_scores_by_dictionary[dict_name] = {}\n",
    "\n",
    "#             for term, score in new_terms_and_scores:\n",
    "#                 if term not in term_counts_and_scores_by_dictionary[dict_name]:\n",
    "#                     term_counts_and_scores_by_dictionary[dict_name][term] = []\n",
    "#                 term_counts_and_scores_by_dictionary[dict_name][term].append(score)\n",
    "\n",
    "#     # save new terms in the curr_dict\n",
    "#     # curr_dicts = combine_ensamble_dicts(term_counts_and_scores_by_dictionary)\n",
    "\n",
    "#     # # Save the new completed dictionary\n",
    "#     # new_dict_name = f\"{time_period[0]}-{time_period[1]}_recursive_dict.json\"\n",
    "#     # new_dict_path = os.path.join(DICTIONARY_SAVE_PATH, new_dict_name)\n",
    "#     # with open(new_dict_path, \"w+\") as f:\n",
    "#     #     json.dump(curr_dicts, f)\n",
    "\n",
    "#     # Clean up memory because for some reason jupyter cant handle it\n",
    "#     del dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presidents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77b3f24db4760b8c61a784a6cf48a467feb1ad85c14ee10af44901428476a873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
