{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.utils import io\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from dataset import CongressDataset\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "DICTIONARY_MAX_SIZE = 100\n",
    "DICTIONARY_MIN_SIZE = 20\n",
    "DICTIONARY_SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "MODEL_SAVE_PATH = os.path.join(\"data\", \"embedding_models\")\n",
    "DICTIONARY_SAVE_PATH = os.path.join(\"data\", \"dictionaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_foundation_words = [\"care\", \"fairness\", \"authority\", \"loyalty\", \"purity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "[(1961, 1964), (1965, 1968), (1969, 1972), (1973, 1976), (1977, 1980), (1981, 1984), (1985, 1988), (1989, 1992), (1993, 1996), (1997, 2000)]\n"
     ]
    }
   ],
   "source": [
    "dictionary_time_periods = [(2007, 2016), (2001, 2006)]\n",
    "\n",
    "start_year = 1873\n",
    "end_year = 2001\n",
    "\n",
    "\n",
    "temp_year = 1873\n",
    "while temp_year < end_year:\n",
    "    dictionary_time_periods.append((temp_year, temp_year + 3))\n",
    "    temp_year += 4\n",
    "\n",
    "print(len(dictionary_time_periods))\n",
    "print(dictionary_time_periods[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_gensim(data_item):\n",
    "    return data_item[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dictionary(model):\n",
    "     moral_foundation_dictionary = {}\n",
    "\n",
    "     for moral_foundation in moral_foundation_words:\n",
    "          top_matching_words = model.wv.most_similar(moral_foundation, topn=DICTIONARY_MAX_SIZE)\n",
    "\n",
    "          # Filter out words that dont meet the cutoff\n",
    "          final_dictionary_words = []\n",
    "          for (word, sim_score) in top_matching_words:\n",
    "               if sim_score > DICTIONARY_SIMILARITY_THRESHOLD:\n",
    "                    final_dictionary_words.append(word)\n",
    "\n",
    "          if len(final_dictionary_words) < DICTIONARY_MIN_SIZE:\n",
    "               dictionary_words_and_scores = model.wv.most_similar(moral_foundation, topn=DICTIONARY_MIN_SIZE)\n",
    "               final_dictionary_words = [word for (word, sim_score) in dictionary_words_and_scores]\n",
    "\n",
    "          moral_foundation_dictionary[moral_foundation] = final_dictionary_words\n",
    "          \n",
    "\n",
    "     return moral_foundation_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CongressDataset(date_range=dictionary_time_periods[3])\n",
    "# dataset.map(format_data_for_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_period = dictionary_time_periods[3]\n",
    "# model = gensim.models.Word2Vec(dataset.data, vector_size=EMBEDDING_SIZE, window=5, min_count=80)\n",
    "# model_save_path = os.path.join(MODEL_SAVE_PATH, f\"{time_period[0]}_{time_period[1]}.model\")\n",
    "# model.save(model_save_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moral_dictionary = calculate_dictionary(model)\n",
    "# dictionary_save_path = os.path.join(DICTIONARY_SAVE_PATH, f\"{time_period[0]}_{time_period[1]}.json\")\n",
    "# with open(dictionary_save_path, \"w+\") as f:\n",
    "#     json.dump(moral_dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary for 2007 - 2016. 3 / 34\n"
     ]
    }
   ],
   "source": [
    "time_period_index = 0\n",
    "for time_period in dictionary_time_periods:\n",
    "    dictionary_save_path = os.path.join(DICTIONARY_SAVE_PATH, f\"{time_period[0]}_{time_period[1]}.json\")\n",
    "    model_save_path = os.path.join(MODEL_SAVE_PATH, f\"{time_period[0]}_{time_period[1]}.model\")\n",
    "\n",
    "    if not os.path.exists(dictionary_save_path) or not os.path.exists(model_save_path):\n",
    "        print(f\"Creating dictionary for {time_period[0]} - {time_period[1]}. {time_period_index} / {len(dictionary_time_periods)}\")\n",
    "        with io.capture_output() as captured:\n",
    "            dataset = CongressDataset(date_range=time_period)\n",
    "            dataset.map(format_data_for_gensim)\n",
    "\n",
    "        model = gensim.models.Word2Vec(dataset.data, vector_size=EMBEDDING_SIZE, window=5, min_count=50)\n",
    "        model.save(model_save_path)\n",
    "\n",
    "        moral_dictionary = calculate_dictionary(model)\n",
    "        with open(dictionary_save_path, \"w+\") as f:\n",
    "            json.dump(moral_dictionary, f)\n",
    "\n",
    "    time_period_index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presidents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77b3f24db4760b8c61a784a6cf48a467feb1ad85c14ee10af44901428476a873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
