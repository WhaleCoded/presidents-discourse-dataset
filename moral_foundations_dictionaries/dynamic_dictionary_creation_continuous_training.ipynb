{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "import csv\n",
    "\n",
    "from dataset import CongressDataset\n",
    "from token_map import TokenMap, create_re_from_formatted_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn;\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "NUM_EPOCHS = 30\n",
    "DICTIONARY_SIMILARITY_THRESHOLD = 0.5\n",
    "WORD_MIN_COUNT = 100\n",
    "NUM_RECURRENCES = 3\n",
    "USE_WIKI = False\n",
    "MAINTAIN_CORE_WORDS = False\n",
    "\n",
    "DICT_ID = \"no_wiki_continuous_v1\"\n",
    "DICTIONARY_SAVE_PATH = os.path.join(os.path.curdir, \"data\", DICT_ID, \"json_dicts\")\n",
    "os.makedirs(DICTIONARY_SAVE_PATH, exist_ok=True)\n",
    "MODELS_SAVE_PATH = os.path.join(os.path.curdir, \"data\", DICT_ID, \"embedding_models\")\n",
    "os.makedirs(MODELS_SAVE_PATH, exist_ok=True)\n",
    "PRETRAINED_MODEL_NAME = \"pretrained_wiki_model.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Fine Tuning\n",
    "LARGE_CLUSTER_WEIGHT = 0.02\n",
    "NUM_ENSAMBLE_RERUNS = 1\n",
    "# NUM_CLUSTERS_TO_CHECK = range(2, 13)\n",
    "# NUM_CLUSTER_RERUNS = 20\n",
    "\n",
    "PREVIOUS_TERM_BOOST = 3\n",
    "NUM_RECURRENCE_BOOST = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMATTED_WIKI_TOKENS_PATH = os.path.join(os.path.curdir, \"data\", DICT_ID, \"formatted_wiki_unique_tokens_corpus.json\")\n",
    "if not os.path.exists(FORMATTED_WIKI_TOKENS_PATH):\n",
    "\n",
    "    wiki_corpus = api.load(\"wiki-english-20171001\")\n",
    "\n",
    "    unique_tokens = set()\n",
    "    formatted_wiki_data = []\n",
    "    for thing in tqdm(wiki_corpus):\n",
    "        for section in thing[\"section_texts\"]:\n",
    "            sentences = nltk.sent_tokenize(section)\n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence)\n",
    "                for word in words:\n",
    "                    unique_tokens.add(word)\n",
    "\n",
    "    wiki_json_format = {\"tokens\": list(unique_tokens)}\n",
    "\n",
    "    del wiki_corpus\n",
    "\n",
    "    with open(FORMATTED_WIKI_TOKENS_PATH, \"w\") as f:\n",
    "        json.dump(wiki_json_format, f)\n",
    "else:\n",
    "    with open(FORMATTED_WIKI_TOKENS_PATH, \"r\") as f:\n",
    "        wiki_json_format = json.load(f)\n",
    "    unique_tokens = wiki_json_format[\"tokens\"]\n",
    "\n",
    "del wiki_json_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token map from disk...\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\n",
    "        os.path.curdir, \"data\", \"cr_speech_sentences_with_speaker_and_date.csv\"\n",
    "    )\n",
    "FORMATTED_DICT_PATH = os.path.join(\n",
    "    os.path.curdir,\n",
    "    \"data\",\n",
    "    DICT_ID, \n",
    "    \"hand_curated\", \n",
    "    \"formatted_hand_curated_dict.json\"\n",
    ")\n",
    "\n",
    "CORE_TERMS_PATH = os.path.join(os.path.curdir,\n",
    "    \"data\",\n",
    "    DICT_ID, \n",
    "    \"hand_curated\", \n",
    "    \"core_terms.json\"\n",
    ")\n",
    "token_map_load_path = os.path.join(os.path.curdir, \"data\", DICT_ID, \"token_map\")\n",
    "\n",
    "dictionary_re = create_re_from_formatted_dictionary(FORMATTED_DICT_PATH)\n",
    "\n",
    "token_map = TokenMap(DATA_PATH, token_map_load_path, dictionary_re=dictionary_re, additional_corpus=unique_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store transformed WIKI CORPUS\n",
    "\n",
    "TRANSFORMED_WIKI_CORPUS_PATH = os.path.join(os.path.curdir, \"data\", DICT_ID, \"transformed_wiki_corpus.csv\")\n",
    "\n",
    "if not os.path.exists(TRANSFORMED_WIKI_CORPUS_PATH):\n",
    "    with open(TRANSFORMED_WIKI_CORPUS_PATH, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        wiki_corpus = api.load(\"wiki-english-20171001\")\n",
    "\n",
    "        for thing in tqdm(wiki_corpus):\n",
    "            for section in thing[\"section_texts\"]:\n",
    "                sentences = nltk.sent_tokenize(section)\n",
    "                for sentence in sentences:\n",
    "                    words = nltk.word_tokenize(sentence)\n",
    "                    writer.writerow([token_map.get_token_id_from_token(word.lower()) for word in words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_path = os.path.join(MODELS_SAVE_PATH, PRETRAINED_MODEL_NAME)\n",
    "\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    print(\"Loading pretrained model\")\n",
    "    model = gensim.models.Word2Vec.load(pretrained_model_path)\n",
    "else:\n",
    "    transformed_wiki_corpus = []\n",
    "    # wiki_corpus = api.load(\"wiki-english-20171001\")\n",
    "    MAX_WIKI_TRAIN_SIZE = 10_000_000\n",
    "    model = None\n",
    "\n",
    "    transformed_wiki_data = []\n",
    "    train_size = 0\n",
    "    with open(TRANSFORMED_WIKI_CORPUS_PATH, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in tqdm(enumerate(reader)):\n",
    "            transformed_wiki_data.append([int(token_id) for token_id in row])\n",
    "\n",
    "            if train_size >= MAX_WIKI_TRAIN_SIZE:\n",
    "                if model == None:\n",
    "                    print(\"Training model\")\n",
    "                    model = gensim.models.Word2Vec(transformed_wiki_data, workers=12, vector_size=EMBEDDING_SIZE, window=5, min_count=WORD_MIN_COUNT)\n",
    "                    model.save(pretrained_model_path)\n",
    "\n",
    "                else:\n",
    "                    print(\"Updating model\")\n",
    "                    model.build_vocab(transformed_wiki_corpus, update=True)\n",
    "                    model.train(transformed_wiki_corpus, total_examples=len(transformed_wiki_corpus), epochs=NUM_EPOCHS)\n",
    "                    model.save(pretrained_model_path)\n",
    "\n",
    "                train_size = 0\n",
    "                transformed_wiki_data = []\n",
    "            train_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[(2001, 2020), (1993, 2000), (1973, 1992), (1953, 1972), (1933, 1952), (1913, 1932), (1893, 1912), (1873, 1892)]\n"
     ]
    }
   ],
   "source": [
    "dictionary_time_periods = []\n",
    "\n",
    "start_year = 1873\n",
    "end_year = 2000\n",
    "\n",
    "\n",
    "temp_year = 1873\n",
    "while temp_year < end_year - 1:\n",
    "    dictionary_time_periods.append((temp_year, temp_year + 19))\n",
    "    temp_year += 20\n",
    "dictionary_time_periods = sorted(dictionary_time_periods, reverse=True)\n",
    "\n",
    "dictionary_time_periods[0] = (1993, 2000)\n",
    "dictionary_time_periods = [(2001, 2020)] + dictionary_time_periods\n",
    "\n",
    "print(len(dictionary_time_periods))\n",
    "print(dictionary_time_periods[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'discrimination', 'judicious', 'chaste', 'poverty', 'obligation', 'forgive', 'suffering', 'cheat', 'obedience', 'defiance', 'disloyal', 'collegial', 'patriotism', 'biased', 'prejudiced', 'disrespect', 'patriot', 'hated', 'belong', 'ethical', 'honest', 'honesty', 'untrustworthy', 'dishonorable', 'compliance', 'crime', 'prideful', 'alienated', 'unfair', 'equity', 'lie', 'American', 'equality', 'belonging', 'authoritative', 'suffered', 'disrespectful', 'corruption', 'illegitimate', 'cheated', 'virtuous', 'pornographic', 'treason', 'lusty', 'agitated', 'forgiven', 'member', 'modesty', 'hateful', 'sexual', 'disrespected', 'impartially', 'membership', 'duty', 'betray', 'suffer', 'impartiality', 'disloyalty', 'pervert', 'partisan', 'leader', 'party', 'wise', 'lust', 'disobey', 'undemocratic', 'betrayal', 'prejudice', 'charity', 'segregated', 'patriotic', 'hurt', 'pornography', 'nation', 'obeyed', 'traitors', 'attacked', 'abuse', 'defy', 'virtue', 'graft', 'compassion', 'belongs', 'authority', 'protect', 'care', 'impartial', 'constitutional', 'betrayed', 'sex', 'civic', 'unjustly', 'caring', 'disobeyed', 'democrat', 'polite', 'inequitably', 'balanced', 'dishonor', 'bribery', 'sympathetic', 'falsehood', 'disgraceful', 'unprofessional', 'unity', 'rightful', 'command', 'unfairly', 'commanded', 'fairly', 'defiant', 'liar', 'republican', 'bribed', 'corrupt', 'justice', 'treasonous', 'lawless', 'loyal', 'leadership', 'disobedient', 'disgrace', 'unbiased', 'legitimate', 'pride', 'agitate', 'moral', 'inequitable', 'obligated', 'integrity', 'amoral', 'abusive', 'untrue', 'injustice', 'sympathy', 'protects', 'cheater', 'obscene', 'forgives', 'crooked', 'equal', 'harmful', 'tradition', 'inequality', 'lied', 'perjury', 'alienate', 'unpatriotic', 'Americans', 'unjust', 'insubordination', 'hierarchy', 'traditionally', 'fairplay', 'rebellion', 'bribe', 'ethics', 'agitation', 'attack', 'ethically', 'oppression', 'hurtful', 'honestly', 'malice', 'traitorous', 'scandalous', 'subversive', 'equals', 'independent', 'nonpartisan', 'obediently', 'hate', 'chastity', 'dishonest', 'truthful', 'harm', 'unethical', 'obey', 'insubordinate', 'lawful', 'heal', 'loyalty', 'bias', 'harmed', 'fatherland'}\n",
      "{0, 12802, 515, 2052, 519, 2057, 3598, 3087, 2585, 4121, 8219, 9756, 1565, 3616, 91681, 13344, 49187, 2085, 16431, 29232, 16949, 4664, 21051, 10301, 6719, 2624, 3136, 4164, 5192, 1609, 7240, 1101, 10832, 6228, 13912, 15964, 19964, 4195, 29796, 57956, 615, 10344, 12909, 2677, 8822, 17529, 7294, 69252, 1680, 3216, 5781, 64153, 3739, 13472, 28324, 17062, 12459, 5294, 18609, 75444, 4796, 16577, 2243, 9414, 14536, 9418, 9422, 14036, 219, 3803, 14044, 23773, 4324, 229, 14565, 10472, 21226, 51949, 19184, 3314, 242, 2292, 52981, 10486, 14583, 15094, 2813, 13059, 23811, 17668, 6405, 3847, 1800, 5897, 12042, 4873, 50444, 2319, 787, 1815, 14106, 9505, 5410, 2857, 15146, 2354, 5939, 30014, 1857, 2369, 15171, 10051, 1349, 5449, 14154, 843, 10571, 16209, 7007, 351, 12642, 13154, 4451, 10082, 16743, 1897, 1899, 11115, 8558, 14703, 4464, 4982, 10615, 32632, 4475, 15742, 42369, 3969, 32139, 17803, 16275, 3987, 2453, 5014, 4503, 8604, 41379, 7077, 15270, 79867, 1962, 20396, 2478, 19377, 5556, 437, 5049, 40378, 5059, 82894, 62927, 10192, 21459, 469, 78295, 7641, 9180, 477, 15840, 21985, 8673, 26083, 32230, 1002, 492, 27116, 3056, 1008, 19445, 12277, 38904, 9722, 1531, 5628}\n"
     ]
    }
   ],
   "source": [
    "# Load the starting dictionaries\n",
    "with open(FORMATTED_DICT_PATH, \"r\") as f:\n",
    "    curr_dicts = json.load(f)\n",
    "\n",
    "with open(CORE_TERMS_PATH, \"r\") as f:\n",
    "    core_terms = json.load(f)\n",
    "    core_terms = set(core_terms[\"core\"])\n",
    "\n",
    "print(core_terms)\n",
    "\n",
    "\n",
    "translated_core_terms = set()\n",
    "for core_term in core_terms:\n",
    "    translated_core_terms.add(token_map.get_token_id_from_token(core_term))\n",
    "\n",
    "print(translated_core_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_gensim(data_item):\n",
    "    return data_item[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sim_score(word, dictionary_terms, model):\n",
    "    tot_sim_score = 0\n",
    "    num_valid_terms = 0\n",
    "\n",
    "    for term in dictionary_terms:\n",
    "        if term != word:\n",
    "            try:\n",
    "                specific_score = model.wv.similarity(word, term)\n",
    "                tot_sim_score += specific_score\n",
    "                num_valid_terms += 1\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "    return tot_sim_score / num_valid_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_sim_score(word, dictionary_terms, model):\n",
    "    all_sim_scores = []\n",
    "\n",
    "    for term in dictionary_terms:\n",
    "        if term != word:\n",
    "            try:\n",
    "                specific_score = model.wv.similarity(word, term)\n",
    "                all_sim_scores.append(specific_score)\n",
    "            except KeyError as e:\n",
    "                #os.path.join(\"data\", \"ensamble_fine_tuning\", \"original_dict.json\")\n",
    "                pass\n",
    "\n",
    "    all_sim_scores = sorted(all_sim_scores)\n",
    "    median_score = all_sim_scores[len(all_sim_scores) // 2]\n",
    "    if word in dictionary_terms:\n",
    "        # Boost the score of words that are already in the dictionary\n",
    "        # print(f\"Boosting {token_map.get_token_from_id(word)} from {median_score} to {median_score * (1 + PREVIOUS_TERM_BOOST)} because it is already in the dictionary\")\n",
    "        median_score = median_score * (1 + PREVIOUS_TERM_BOOST)\n",
    "\n",
    "    return median_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(not USE_WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dictionary(model, dictionary_terms, previous_dictionary_size: int, core_terms: set):\n",
    "     new_term_counts = {}\n",
    "     new_term_scores = {}\n",
    "\n",
    "     for term in dictionary_terms:\n",
    "          try:\n",
    "               top_matching_words = model.wv.most_similar(term, topn=previous_dictionary_size)\n",
    "\n",
    "               if not USE_WIKI:\n",
    "                    top_matching_words = [(word, sim_score) \n",
    "                                         for word, sim_score in top_matching_words\n",
    "                                         if not token_map.token_id_unique_to_additional_corpus(word)]\n",
    "                    if len(top_matching_words) < previous_dictionary_size:\n",
    "                         print(f\"We removed {previous_dictionary_size - len(top_matching_words)} words.\")\n",
    "\n",
    "               for (word, sim_score) in top_matching_words:\n",
    "                    # Keep track of max sim score for each word 0\n",
    "\n",
    "                    \n",
    "                    if not (word in new_term_scores):\n",
    "                         # Only needs to be calculated once\n",
    "                         # average_sim_score = calculate_average_sim_score(word, dictionary_terms, model)\n",
    "                         median_score = calculate_median_sim_score(word, dictionary_terms, model)\n",
    "                         new_term_scores[word] = median_score\n",
    "                    else:\n",
    "                         # Boost the score of words that appear more times\n",
    "                         # new_term_scores[word] = new_term_scores[word] * (1 + NUM_RECURRENCE_BOOST)\n",
    "                         original_score = new_term_scores[word]\n",
    "                         # median_score = calculate_median_sim_score(word, dictionary_terms, model)\n",
    "                         new_term_scores[word] = original_score * (1 + NUM_RECURRENCE_BOOST)\n",
    "                         \n",
    "\n",
    "                    # Keep track of how many times a word appears in the top matching words\n",
    "                    # if sim_score > DICTIONARY_SIMILARITY_THRESHOLD:\n",
    "                    #      if word not in new_term_counts:\n",
    "                    #           new_term_counts[word] = 0\n",
    "                    #      new_term_counts[word] += 1\n",
    "                         \n",
    "          except KeyError as e:\n",
    "               translated_term = token_map.get_token_from_id(term)\n",
    "               # print(f\"{translated_term}-{term}: {e}\")\n",
    "\n",
    "     # print(\"Previous dictionary reoccurrences:\")\n",
    "     # print({k: v for k, v in sorted(previous_dict_reocurrences.items(), key=lambda item: item[0])})\n",
    "     # Sort dictionaries and remove words that don't appear enough times\n",
    "     # new_term_counts = {k: v for k, v in sorted(new_term_counts.items(), key=lambda item: item[1], reverse=True) if v >= NUM_RECURRENCES}     \n",
    "     new_term_scores = {k: v for k, v in sorted(new_term_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "\n",
    "     # Compile the dictionary from the recurring words\n",
    "     final_dictionary_scores = {}\n",
    "     final_dictionary_terms = set()\n",
    "     if MAINTAIN_CORE_WORDS:\n",
    "          core_terms_present = set()\n",
    "          for term in dictionary_terms:\n",
    "               if term in core_terms:\n",
    "                    core_terms_present.add(term)\n",
    "                    final_dictionary_scores[term] = math.inf\n",
    "          final_dictionary_terms = core_terms_present\n",
    "     # for term in new_term_counts.keys():\n",
    "     #      if len(final_dictionary_terms) >= previous_dictionary_size:\n",
    "     #           break\n",
    "     #      final_dictionary_terms.add(term)\n",
    "\n",
    "     # Add words based on similarity score until the dictionary is full\n",
    "     for term, score in new_term_scores.items():\n",
    "          if term in final_dictionary_terms:\n",
    "               continue\n",
    "          \n",
    "          if len(final_dictionary_terms) >= previous_dictionary_size:\n",
    "               break\n",
    "          final_dictionary_terms.add(term)\n",
    "          final_dictionary_scores[term] = score\n",
    "     \n",
    "     # Remove the unknown token if it is in the dictionary\n",
    "     # The unkown token is meaningless\n",
    "     if token_map.unkown_id in final_dictionary_terms:\n",
    "          final_dictionary_terms.remove(token_map.unkown_id)\n",
    "          final_dictionary_scores.pop(token_map.unkown_id)\n",
    "\n",
    "     for term in final_dictionary_terms:\n",
    "          if token_map.get_token_from_id(term) == token_map.unkown_token:\n",
    "               print(\"Removed unknown token from dictionary\")\n",
    "               final_dictionary_scores.pop(term)\n",
    "               final_dictionary_terms.remove(term)\n",
    "\n",
    "     # print(f\"Previous dictionary reocurrences: {sorted([token_map.get_token_from_id(term) for term in final_dictionary_terms if term in dictionary_terms]) }\")\n",
    "     # print(f\"Average dictionary_sim_score for with previous boost: {sum(final_dictionary_scores.values()) / len(final_dictionary_scores)}\")\n",
    "     # print(f\"Average score of top 25 terms: {sum(sorted(final_dictionary_scores.values(), reverse=True)[:25]) / 25}. There are {len(final_dictionary_scores)} terms in the dictionary.\")\n",
    "\n",
    "     return list(final_dictionary_terms), final_dictionary_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_of_scores_to_final_scores(list_scores_dict):\n",
    "    \"\"\"\n",
    "    Returns a list of terms translated to english sorted by their values in descending order\n",
    "    \"\"\"\n",
    "    final_scores = {}\n",
    "\n",
    "    total_num_recurrences = 0\n",
    "    for term, score_lists in list_scores_dict.items():\n",
    "        num_recurrences = len(score_lists)\n",
    "        if num_recurrences > 1:\n",
    "            total_num_recurrences += 1\n",
    "\n",
    "        # Each term with multiple occurences gets a percentage boost over the average score\n",
    "        # TODO: Consider using the median here instead of the average\n",
    "        # if num_recurrences > 1:\n",
    "        #     print(f\"Term {token_map.get_token_from_id(term)} has {num_recurrences} recurrences\")\n",
    "    \n",
    "        final_scores[term] = (sum(score_lists) / num_recurrences) * (1 + (num_recurrences * NUM_RECURRENCE_BOOST))\n",
    "        \n",
    "    final_dictionary = [token_map.get_token_from_id(k) for k, v in sorted(final_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "    final_scores = [v for k, v in sorted(final_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "    return final_dictionary, final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_ensamble_dicts(ensamble_dicts, desired_dict_sizes):\n",
    "    combined_dict = {}\n",
    "\n",
    "    for moral_foundation, dict_terms in ensamble_dicts.items():\n",
    "        # Convert list of scores to final scores\n",
    "        sorted_terms, sorted_scores = convert_list_of_scores_to_final_scores(dict_terms)\n",
    "        final_dict_terms = sorted_terms[:desired_dict_sizes[moral_foundation]]\n",
    "        final_dict_scores = sorted_scores[:desired_dict_sizes[moral_foundation]]\n",
    "\n",
    "        # print(f\"Final dictionary size for {moral_foundation}: {len(final_dict_terms)}\")\n",
    "        # print(f\"Average score for {moral_foundation}: {sum(final_dict_scores) / len(final_dict_scores)}\")\n",
    "\n",
    "        combined_dict[moral_foundation] = final_dict_terms\n",
    "\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time period is (2001, 2020).  1/8\n",
      "The current time period is (1993, 2000).  2/8\n",
      "The current time period is (1973, 1992).  3/8\n",
      "The current time period is (1953, 1972).  4/8\n",
      "The current time period is (1933, 1952).  5/8\n",
      "The current time period is (1913, 1932).  6/8\n",
      "The current time period is (1893, 1912).  7/8\n",
      "The current time period is (1873, 1892).  8/8\n"
     ]
    }
   ],
   "source": [
    "for i, time_period in enumerate(dictionary_time_periods):\n",
    "    term_counts_and_scores_by_dictionary = {}\n",
    "    desired_dict_sizes = {}\n",
    "    num_trained_models = 0\n",
    "    tot_train_time = 0\n",
    "    print(f\"The current time period is {time_period}.  {i+1}/{len(dictionary_time_periods)}\")\n",
    "    model_save_path = os.path.join(MODELS_SAVE_PATH, f\"{time_period[0]}_{time_period[1]}.model\")\n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        # Load the dataset\n",
    "        middle_date = (time_period[0] + time_period[1]) // 2\n",
    "        first_time_period = (time_period[0], middle_date)\n",
    "        second_time_period = (middle_date, time_period[1])\n",
    "        dataset = CongressDataset(token_map=token_map, date_range=first_time_period)\n",
    "        dataset.map(format_data_for_gensim)\n",
    "\n",
    "        # start_time = time.time()\n",
    "        model.build_vocab(dataset.data, update=True)\n",
    "        model.train(dataset.data, epochs=NUM_EPOCHS, total_examples=len(dataset.data))\n",
    "\n",
    "        dataset = CongressDataset(token_map=token_map, date_range=second_time_period)\n",
    "        dataset.map(format_data_for_gensim)\n",
    "\n",
    "        # start_time = time.time()\n",
    "        model.build_vocab(dataset.data, update=True)\n",
    "        model.train(dataset.data, epochs=NUM_EPOCHS, total_examples=len(dataset.data))\n",
    "        # model = gensim.models.Word2Vec(dataset.data, vector_size=EMBEDDING_SIZE, window=5, min_count=WORD_MIN_COUNT, workers=12)\n",
    "        # tot_train_time += time.time() - start_time\n",
    "        model.save(model_save_path)\n",
    "        del dataset\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec.load(model_save_path)\n",
    "\n",
    "    for dict_name in curr_dicts.keys():\n",
    "        if dict_name not in term_counts_and_scores_by_dictionary:\n",
    "            term_counts_and_scores_by_dictionary[dict_name] = {}\n",
    "\n",
    "        # grab existing terms\n",
    "        dict_terms = curr_dicts[dict_name]\n",
    "        desired_dict_sizes[dict_name] = len(dict_terms)\n",
    "        translated_terms = [token_map.get_token_id_from_token(term) for term in dict_terms if token_map.get_token_id_from_token(term) in model.wv]\n",
    "        \n",
    "        dictionary_embeddings = [embedding for embedding in model.wv[translated_terms]]\n",
    "\n",
    "        # Calculate the centroids of the clusters\n",
    "        # num_clusters, cluster_labels = calculate_cluster_centroids(dictionary_embeddings)\n",
    "        # cluster_distributions = calculate_cluster_distributions(cluster_labels, num_clusters)\n",
    "        # print(cluster_distributions)\n",
    "        # translated_terms_and_labels = list(zip(translated_terms, cluster_labels))\n",
    "\n",
    "        # Calculate the new dictionary terms\n",
    "        new_terms, new_term_scores = calculate_dictionary(model, translated_terms, len(dict_terms), translated_core_terms)\n",
    "\n",
    "        for term, score in new_term_scores.items():\n",
    "            if term not in term_counts_and_scores_by_dictionary[dict_name]:\n",
    "                term_counts_and_scores_by_dictionary[dict_name][term] = []\n",
    "            term_counts_and_scores_by_dictionary[dict_name][term].append(score)\n",
    "\n",
    "    curr_dicts = combine_ensamble_dicts(term_counts_and_scores_by_dictionary, desired_dict_sizes)\n",
    "\n",
    "    # Save the new completed dictionary\n",
    "    new_dict_name = f\"{time_period[0]}-{time_period[1]}_recursive_dict.json\"\n",
    "    new_dict_path = os.path.join(DICTIONARY_SAVE_PATH, new_dict_name)\n",
    "    with open(new_dict_path, \"w+\") as f:\n",
    "        json.dump(curr_dicts, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presidents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77b3f24db4760b8c61a784a6cf48a467feb1ad85c14ee10af44901428476a873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
