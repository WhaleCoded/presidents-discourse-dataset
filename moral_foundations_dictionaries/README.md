# Generate Moral Foundation Dictionaries
The code in this folder can be used to format data used to train word embedding models, train word embedding models using an ensemble or pretrained models, generate semantic shift sensitive moral foundation dictionaries, analyse the makeup of those dictionaries, and then generate graphs to describe the makeup and properties of those dictionaries.


## Getting Started
* You will need to obtain the data you will be using to train your word embedding models. I used the data provided by Stanford, but you can use any text based dataset.
* Format the data into groups of tokenized sentences. Run the format_tokenize_congressional_data.py followed by the format_add_metadata_to_tokenized_speeches.py to format and store the data in a way that can be used by the word embedding models.
* Choose how you want to generate dynamic moral foundation dictionaries. There are two methods available. One uses an ensemble of moral foundation dictionaries to make more intuitive decisions about what terms get included in the dictionary. The other uses a uses wikipedia data and repeatedly finetunes the same model to provide more accurate embeddings. (Eventually, I plan on combining these two methods together)
* Run either the dynamic_dictionary_creation_ensemble.ipynb or the dynamic_dictionary_creation_continuos_training.ipynb scripts to generate the dynamic moral foundation dictionaries depending on your decision from the previous step.
* You can then run the analyze_dictionary_demographics to calculate how the dictinaries changed over time and between each other.
* Finally, you can the plot_analysis.ipynb script to generate graphs that describe the makeup and properties of the dictionaries.

## Results
    The results from the final version of moral foundation dictionaries can be found in the results folder under the core_no_wiki_continuous_v1 sub folder. These results were generated using the `dynamic_dictionary_creation_continuous_training.ipynb` script. Core terms were included and tokens unique to wikipedia were removed.

## Data
    There are three datasets used in this folder: the presidential data that can be scraped using the code found in presidential_data_scrape folder, congressional speeches from between 2016 and 1873 shared by Stanford, and the wikipedia dataset provided by the python library Gensim.

    I am not at liberty to distribute the data I recieved from Stanford, but the code can be easily adapted to work with any text based dataset.


## Index
* `analyze_dictionary_demographics.ipynb` - This script looks at a set of moral foundation dictionaries and calculates how many and what terms change between each dictionary. Results will get saved to csv_analysis folder at the location specified.
* `dataset.py` - This is a utility class that is used to load congressional data into memory. This class can also be used to map over and split the data.
* `dynamic_dictionary_creation_continuous_training.ipynb` - This script will generate a set of dynamic moral dictionaries by repeatedly finetuning the same model as we generate dictionaries for different time periods. In order for this script to work, the congressional data must already by formatted and saved (see the Getting Started section for more details). The script provides the option require core terms to be preserved and include wikipedia unique tokens.
* `dynamic_dictionary_creation_ensemble.ipynb` - This script will generate a set of dynamic moral dictionaries by using an ensemble of moral foundation dictionaries to make more intuitive decisions about what terms get included in the dictionary. In order for this script to work, the congressional data must already by formatted and saved (see the Getting Started section for more details).
* `format_add_metadata_to_tokenized_speeches.py` - This script will add metadata to the tokenized speeches created by the `format_tokenize_congressional_data.py` script. This metadata will be used to split the speeches into different time periods.
* `format_convert_json_dicts_to_csv.py` - This script will convert the json dictionaries generated by the dynamic_dictionary_creation scripts into csv files that are a little easier to read.
* `format_convert_raw_static_dict_to_formatted_w_core.py` - This script converts teh static hand curated dictionary into a format that can be used by the dynamic_dictionary_creation scripts.
* `format_tokenize_congressional_data.py` - This script will format and tokenize the congressional data provided by Stanford. The data will be saved in a format that can be used by the word embedding models.
* `plot_analysis.ipynb` - This script will generate graphs that describe the makeup and properties of the dictionaries. In order for this script to work, the `analyze_dictionary_demographics.ipynb` must already have been run on a set of dynamic dictionaries.
* `plot_core_prop.ipynb` - This script will create and save a graph that depicts the proportion of core terms for a given dictionary.
* `plot_dictionary_stability.ipynb` - This script will create a graph that depicts teh proportion of terms that have changed between two dictionaries. This is meant to be used to compare a dictionary with a predecessor dictionary generated using the `test_stability.ipynb` script. In order for this script to work, you must run the `test_diff_two_dicts.ipynb` script on the two dictionaries you want to compare.
* `plot_tsne_and_pca_of_president_frequency_distributions.ipynb` - This script uses frequency counts calculated using teh dynamic dictionaries to run a PCA and TSNE analysis on the moral foundation data. Each point on the represents a president and is labeled with the president's name. The points are colored based on the time period the president was in office.
* `test_diff_two_dicts.ipynb` - This script compares what words are different and the same between two dictionaries. The corresponding proportions, counts, and term lists are calculated and saved.
* `test_reverse_generation` - This script starts with a moral foundation dictionary for the time period 1873 and then generates a new dictionary for each time period going forward. This was used to test the effectiveness and realiaabilty of the our method.
* `test_stability.ipynb` - This script takes a dictionary normally the one for the 2020-2000 time period and then generates a new dictionary for the same time period. This process can be repeated any number of times specified, but normally I do it 20 times. This was used to test the stability of our method.
* `token_map.py` - This is a utility class that helps reduce the size and memory required to train the word embedding models. This class maps each token to an integer and then uses that integer to represent the token in the word embedding model. Integers are much smaller than strings so this saves quite a bit of memory. (This class also uses frequency when assigning integers to tokens so that the most frequent tokens get the smallest integers thus saving more space)